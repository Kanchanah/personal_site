\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Application of Feature Selection with Annealing to Regression Problem},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{Application of Feature Selection with Annealing to Regression Problem}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{}
  \preauthor{}\postauthor{}
  \date{}
  \predate{}\postdate{}


\begin{document}
\maketitle

Feature Selection is a well known preprocessing step for high
dimensional data. This project focuses on a novel method, Feature
Selection with Annealing, for selecting features. One of the biggest
advantage of this method over others are that this method takes a
shorter time for convergence. This happens because while the model
parameters are updated, the variables are also removed at each iteration
while reducing the problem size.

\section{The Feature Selection with Annealing Algorithm}

\noindent
Let L(\(\beta\),\(\tau\)) be the loss function and
(\(x_i,y_i\)),\(i=1,N\) be the training examples with
\(x_i\in\mathbb{R}^{M}\) The formulation of the selected problem will be
constrained optimization.

\[\beta=argmin_{\substack{|\{ j : \beta_j\neq 0\}|\leq k }}L(\beta,\tau)\]
\noindent
where k is the number of relevant features and this is a given parameter
and the loss function L(\(\beta\),\(\tau\)) is differentiable with
respect to \(\beta\) and \(\tau\).

\noindent
The algorithm's design will be as given below

\textbf{Algorithm: Feature Selection with Annealing (FSA)}

\textbf{Input}: Training examples (\(x_i,y_i\)),\(i=1,N\)
\textbf{Output}: Trained classifier parameter vector \(\beta\) 1:
Initialize \(\beta\)=0 2: \textbf{for} e=1 to \(N^{iter}\) do 3:
~~~~~~~~Update
\(\beta\leftarrow \beta-\eta\frac{dL(\beta,\tau)}{d\beta}\) 4:
~~~~~~~~Keep only the \(M_e\) variables with the highest
\textbar{}\(\beta_j\)\textbar{} and renumber them 1\ldots{}.\(M_e\) 5:
\textbf{end for}

\noindent
The support set of the coefficient vector is gradually tightened till we
reach \(|\{j,\beta_j \neq0)\}|\lesssim k\). Hence from the algorithm it
can be seen that the problem size and complexity drops at each iteration
as we are removing the variables at each iteration. This is considered
one of the most attractive features of this algorithm\newline

\noindent
A slow annealing schedule works slow enough for estimation and selection
accuracy. However a decaying schedule could reduce the computational
cost as mentioned.\newline

\hspace{0.5cm}

\(M_e=k+(M-k)max\big(0,\frac{N^{iter}-2e}{2e\mu+N^{iter}}\big)\),
e=\({1,N^{iter}}\)\newline

Here note than \(\mu\) refers to the annealing parameter and \(\eta\)
refers to the gradient learning rate. Althought a range of \(\eta\) and
\(\mu\) values can be used, for the case of this analysis, the
convergence occurs pretty quickly for \(\mu\)=10 and \(\eta\)=2 and
these will be the values I used in my analysis.

\subsection{The Problem}\label{the-problem}

Feature Selection will be done on the regression problem given below.

Here \(\textbf X_{ij}\sim uniform(-2,2)\) and \(\epsilon\sim N(0,1)\).
\(\beta_j^*\), the true coefficient vector, has been initialized to be 1
for every 10th value. \(\tau_j^*\), the true coefficient vector, has
been initialized to be 0 for all elements.

\(f(\textbf{X})=max(Relu(X_{j}-\tau_j\))\(\beta_j\)), j=1,\ldots{},M

\(y_i\)=\(f(\textbf{X})+\epsilon\)\newline

\noindent
Since this is a regression problem, we will use the squared error loss
L(\(\beta,\tau\))=\(\sum_{i=1}^{N} (y_i-f(\textbf{X}))^2.\)\newline

\subsubsection{Results}\label{results}

Figures 1-4 below indicate the results for N=3000 and M=100. As seen
from the results it is seen that the most of \(\beta_j\) and \(\tau_j\)
are zeroed and only 10 for each parameter have values other than 0. The
cost function reduces to a constant. However, the histogram indicates
that the number of chosen betas that coincide with the initial betas
were mainly clustered around 80-90\%.\newline

\noindent
The number of observations are increased to observe the changes that
occurs to the Figures. The parameters set for the next part of the
analysis are given as: N=5000 and M=100. As seen from the results in
Figures 5-8, the number of the \(\beta_j\) that coincides with the
initial \(\beta_j^*\) has increased as seen from the histogram.\newline

\noindent
The number of observations are further increased. The parameters set for
the next part of the analysis are given as: N=8000 and M=100. The
Figures 9-12 indicate that all the \(\beta_j\) coincide with the the
initial \(\beta_j^*\).


\end{document}
